{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading datasets\n",
    "i'll start by loading the datasets into pandas dataframes, and then i'll perform some basic cleaning and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# define the path to the data\n",
    "path_to_data = '../data/raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  sentiment\n",
      "0  An inspiration in all aspects: Fashion, fitnes...          1\n",
      "1  :)KISSES TheFashionIcon,Apka Apna Awam Ka Chan...          1\n",
      "2  Can you donate?,Omg he... kissed... him crying...          1\n",
      "3  love love happy,thanks happy,C'mon Tweeps, Joi...          1\n",
      "4                                Do spread the word.          1\n",
      "(587, 2)\n",
      "                                                text  sentiment\n",
      "0  How unhappy  some dogs like it though,talking ...         -1\n",
      "1  I got some money  I need to change into R but ...         -1\n",
      "2  unhappy ,it's that A*dy guy from pop Asia and ...         -1\n",
      "3                            Is this how I find out.         -1\n",
      "4                                Everyone knows now.         -1\n",
      "(357, 2)\n",
      "                                                text  sentiment\n",
      "0  Pak PM survives removal scare, but court order...          0\n",
      "1  ,Supreme Court quashes criminal complaint agai...          0\n",
      "2  ,FCRA slap on NGO for lobbying...But was it do...          0\n",
      "3  ,Why doctors, pharma companies are opposing na...          0\n",
      "4  His officer learnt ground reality -- and  a di...          0\n",
      "(684, 2)\n"
     ]
    }
   ],
   "source": [
    "# read the csv files content\n",
    "with open(os.path.join(path_to_data, 'processedPositive.csv'), 'r') as file:\n",
    "    happy_content = file.read()\n",
    "with open(os.path.join(path_to_data, 'processedNegative.csv'), 'r') as file:\n",
    "    sad_content = file.read()\n",
    "with open(os.path.join(path_to_data, 'processedNeutral.csv'), 'r') as file:\n",
    "    neutral_content = file.read()\n",
    "\n",
    "# split the content into individual tweets\n",
    "happy_sentences = sent_tokenize(happy_content)\n",
    "happy_tweets = pd.DataFrame(happy_sentences, columns=['text'])\n",
    "happy_tweets['sentiment'] = 1\n",
    "sad_sentences = sent_tokenize(sad_content)\n",
    "sad_tweets = pd.DataFrame(sad_sentences, columns=['text'])\n",
    "sad_tweets['sentiment'] = -1\n",
    "neutral_sentences = sent_tokenize(neutral_content)\n",
    "neutral_tweets = pd.DataFrame(neutral_sentences, columns=['text'])\n",
    "neutral_tweets['sentiment'] = 0\n",
    "\n",
    "print(happy_tweets.head())\n",
    "print(happy_tweets.shape)\n",
    "print(sad_tweets.head())\n",
    "print(sad_tweets.shape)\n",
    "print(neutral_tweets.head())\n",
    "print(neutral_tweets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  merge and clean the data\n",
    "in this step we'll clean the data by removing the duplicates and stop words, which are often meaningless words that can add noise to the dataset rather than meaningful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  sentiment\n",
      "0  An inspiration in all aspects: Fashion, fitnes...          1\n",
      "1  :)KISSES TheFashionIcon,Apka Apna Awam Ka Chan...          1\n",
      "2  Can you donate?,Omg he... kissed... him crying...          1\n",
      "3  love love happy,thanks happy,C'mon Tweeps, Joi...          1\n",
      "4                                Do spread the word.          1\n",
      "(1628, 2)\n"
     ]
    }
   ],
   "source": [
    "merged_tweets = pd.concat([happy_tweets, sad_tweets, neutral_tweets], ignore_index=True)\n",
    "merged_tweets.dropna(inplace=True)\n",
    "merged_tweets.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(merged_tweets.head())\n",
    "print(merged_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shrooma/miniconda3/envs/tweets/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  sentiment\n",
      "0  inspiration aspects: fashion, fitness, beauty ...          1\n",
      "1  :)kisses thefashionicon,apka apna awam ka chan...          1\n",
      "2  donate?,omg he... kissed... crying joy,happy a...          1\n",
      "3  love love happy,thanks happy,c'mon tweeps, joi...          1\n",
      "4                                       spread word.          1\n",
      "(1533, 2)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# transform the text to lowercase\n",
    "merged_tweets['text'] = merged_tweets['text'].str.lower()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# remove stop words\n",
    "merged_tweets['text'] = merged_tweets['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords.words('english')]))\n",
    "\n",
    "# remove duplicates\n",
    "merged_tweets.drop_duplicates(subset=['text'], inplace=True)\n",
    "merged_tweets.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(merged_tweets.head())\n",
    "print(merged_tweets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split the data with stratification\n",
    "making a split on the train and test (20%) datasets with stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1226, 2)\n",
      "(307, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_tweets, test_tweets = train_test_split(merged_tweets,\n",
    "                                             test_size=0.2,\n",
    "                                             stratify=merged_tweets['sentiment'],\n",
    "                                             random_state=42)\n",
    "\n",
    "print(train_tweets.shape)\n",
    "print(test_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset:\n",
      "sentiment\n",
      " 0    0.397913\n",
      " 1    0.371168\n",
      "-1    0.230920\n",
      "Name: proportion, dtype: float64\n",
      "Train dataset:\n",
      "sentiment\n",
      " 0    0.398042\n",
      " 1    0.371126\n",
      "-1    0.230832\n",
      "Name: proportion, dtype: float64\n",
      "Test dataset:\n",
      "sentiment\n",
      " 0    0.397394\n",
      " 1    0.371336\n",
      "-1    0.231270\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# check the distribution of the sentiment in the original, train and test datasets\n",
    "print(\"Original dataset:\")\n",
    "print(merged_tweets['sentiment'].value_counts(normalize=True))\n",
    "print(\"Train dataset:\")\n",
    "print(train_tweets['sentiment'].value_counts(normalize=True))\n",
    "print(\"Test dataset:\")\n",
    "print(test_tweets['sentiment'].value_counts(normalize=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess and vectorize the data\n",
    "\n",
    "in this step i'll prepare multiple datasets, each dataset is a combination of a preprocessing approach and a vectorization approach.\n",
    "\n",
    "-- preprocessing approaches :\n",
    "- stemming\n",
    "- lemmatization\n",
    "- stemming + mispellings correction\n",
    "- lemmatization + mispellings correction\n",
    "\n",
    "-- vectorization approaches :\n",
    "- binary vectorization\n",
    "- word counts\n",
    "- tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# import the preprocessing and vectorization functions\n",
    "from utils.text_preprocessing import stem_text, lemmatize_text, correct_spelling\n",
    "from utils.text_vectorization import binary_vectorizer, count_vectorizer, tfidf_vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionnary to store the datasets\n",
    "datasets = {}\n",
    "vectorizers = {}\n",
    "\n",
    "# preprocessing and vectorization dictionary\n",
    "preprocessing_methods = {\n",
    "    'stemming' : stem_text,\n",
    "    'lemmatization' : lemmatize_text,\n",
    "    'stemming_misspelling_correction' : lambda x: correct_spelling(stem_text(x)),\n",
    "    'lemmatization_misspelling_correction' : lambda x: correct_spelling(lemmatize_text(x))\n",
    "}\n",
    "\n",
    "vectorization_methods = {\n",
    "    'binary' : binary_vectorizer,\n",
    "    'word_counts' : count_vectorizer,\n",
    "    'tf-idf' : tfidf_vectorizer\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['stemming_binary', 'stemming_binary_test', 'stemming_word_counts', 'stemming_word_counts_test', 'stemming_tf-idf', 'stemming_tf-idf_test', 'lemmatization_binary', 'lemmatization_binary_test', 'lemmatization_word_counts', 'lemmatization_word_counts_test', 'lemmatization_tf-idf', 'lemmatization_tf-idf_test', 'stemming_misspelling_correction_binary', 'stemming_misspelling_correction_binary_test', 'stemming_misspelling_correction_word_counts', 'stemming_misspelling_correction_word_counts_test', 'stemming_misspelling_correction_tf-idf', 'stemming_misspelling_correction_tf-idf_test', 'lemmatization_misspelling_correction_binary', 'lemmatization_misspelling_correction_binary_test', 'lemmatization_misspelling_correction_word_counts', 'lemmatization_misspelling_correction_word_counts_test', 'lemmatization_misspelling_correction_tf-idf', 'lemmatization_misspelling_correction_tf-idf_test'])\n"
     ]
    }
   ],
   "source": [
    "for prep_name, prep_func in preprocessing_methods.items():\n",
    "    # preprocess the train and test data\n",
    "    train_preprocessed = train_tweets['text'].apply(prep_func)\n",
    "    test_preprocessed = test_tweets['text'].apply(prep_func)\n",
    "\n",
    "    for vec_name, vec_fund in vectorization_methods.items():\n",
    "        train_vectorized, vectorizer = vec_fund(train_preprocessed)\n",
    "        test_vectorized = vectorizer.transform(test_preprocessed)\n",
    "\n",
    "        # store the datasets with labels\n",
    "        dataset_key = f'{prep_name}_{vec_name}'\n",
    "        datasets[dataset_key] = (train_vectorized, train_tweets['sentiment'])\n",
    "        datasets[f'{dataset_key}_test'] = (test_vectorized, test_tweets['sentiment'])\n",
    "\n",
    "        vectorizers[dataset_key] = vectorizer\n",
    "\n",
    "print(datasets.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, we'll try to see the content of each dataset in the dataset dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: stemming_binary\n",
      "Data : (1226, 4563)\n",
      "Labels : [ 1  0 -1]\n",
      "Dataset: stemming_binary_test\n",
      "Data : (307, 4563)\n",
      "Labels : [ 1 -1  0]\n",
      "Dataset: stemming_word_counts\n",
      "Data : (1226, 4563)\n",
      "Labels : [ 1  0 -1]\n",
      "Dataset: stemming_word_counts_test\n",
      "Data : (307, 4563)\n",
      "Labels : [ 1 -1  0]\n",
      "Dataset: stemming_tf-idf\n",
      "Data : (1226, 4563)\n",
      "Labels : [ 1  0 -1]\n",
      "Dataset: stemming_tf-idf_test\n",
      "Data : (307, 4563)\n",
      "Labels : [ 1 -1  0]\n",
      "Dataset: lemmatization_binary\n",
      "Data : (1226, 4803)\n",
      "Labels : [ 1  0 -1]\n",
      "Dataset: lemmatization_binary_test\n",
      "Data : (307, 4803)\n",
      "Labels : [ 1 -1  0]\n",
      "Dataset: lemmatization_word_counts\n",
      "Data : (1226, 4803)\n",
      "Labels : [ 1  0 -1]\n",
      "Dataset: lemmatization_word_counts_test\n",
      "Data : (307, 4803)\n",
      "Labels : [ 1 -1  0]\n",
      "Dataset: lemmatization_tf-idf\n",
      "Data : (1226, 4803)\n",
      "Labels : [ 1  0 -1]\n",
      "Dataset: lemmatization_tf-idf_test\n",
      "Data : (307, 4803)\n",
      "Labels : [ 1 -1  0]\n",
      "Dataset: stemming_misspelling_correction_binary\n",
      "Data : (1226, 4563)\n",
      "Labels : [ 1  0 -1]\n",
      "Dataset: stemming_misspelling_correction_binary_test\n",
      "Data : (307, 4563)\n",
      "Labels : [ 1 -1  0]\n",
      "Dataset: stemming_misspelling_correction_word_counts\n",
      "Data : (1226, 4563)\n",
      "Labels : [ 1  0 -1]\n",
      "Dataset: stemming_misspelling_correction_word_counts_test\n",
      "Data : (307, 4563)\n",
      "Labels : [ 1 -1  0]\n",
      "Dataset: stemming_misspelling_correction_tf-idf\n",
      "Data : (1226, 4563)\n",
      "Labels : [ 1  0 -1]\n",
      "Dataset: stemming_misspelling_correction_tf-idf_test\n",
      "Data : (307, 4563)\n",
      "Labels : [ 1 -1  0]\n",
      "Dataset: lemmatization_misspelling_correction_binary\n",
      "Data : (1226, 4803)\n",
      "Labels : [ 1  0 -1]\n",
      "Dataset: lemmatization_misspelling_correction_binary_test\n",
      "Data : (307, 4803)\n",
      "Labels : [ 1 -1  0]\n",
      "Dataset: lemmatization_misspelling_correction_word_counts\n",
      "Data : (1226, 4803)\n",
      "Labels : [ 1  0 -1]\n",
      "Dataset: lemmatization_misspelling_correction_word_counts_test\n",
      "Data : (307, 4803)\n",
      "Labels : [ 1 -1  0]\n",
      "Dataset: lemmatization_misspelling_correction_tf-idf\n",
      "Data : (1226, 4803)\n",
      "Labels : [ 1  0 -1]\n",
      "Dataset: lemmatization_misspelling_correction_tf-idf_test\n",
      "Data : (307, 4803)\n",
      "Labels : [ 1 -1  0]\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, (vectorized_data, labels) in datasets.items():\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(\"Data :\", vectorized_data.shape)\n",
    "    print(\"Labels :\", labels.unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: text         ,from may 1, vehicles: gadkari.,no cars pm, mi...\n",
      "sentiment                                                    0\n",
      "Name: 939, dtype: object\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "first_dataset_name = list(datasets.keys())[0]\n",
    "vectorized_data, labels = datasets[first_dataset_name]\n",
    "\n",
    "index = 384\n",
    "\n",
    "original_text = train_tweets.iloc[index]\n",
    "label = labels.iloc[index]\n",
    "\n",
    "print(f\"Original Text: {original_text}\")\n",
    "print(f\"Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top 10 similar tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_n_similar_tweets(vectorized_data, n=10):\n",
    "    # compute cosine similarity matrix\n",
    "    similarity_matrix = cosine_similarity(vectorized_data)\n",
    "    \n",
    "    # set the diagonal of the similarity matrix to 0 to ignore self-similarity\n",
    "    np.fill_diagonal(similarity_matrix, 0)\n",
    "\n",
    "    # since the matrix is symmetric, we need one triangle (upper or lower)\n",
    "    num_tweets = similarity_matrix.shape[0]\n",
    "    upper_triangle_indices = np.triu_indices(num_tweets, k=1)\n",
    "    flat_similarities = similarity_matrix[upper_triangle_indices]\n",
    "\n",
    "    # get the indices of the top 10 most similar tweet pairs\n",
    "    top_10_indices_flat = np.argsort(flat_similarities)[-10:]\n",
    "    top_10_pairs = [(upper_triangle_indices[0][i], upper_triangle_indices[1][i]) for i in top_10_indices_flat]\n",
    "\n",
    "    # print the indices and similarity scores of the top 10 most similar tweet pairs\n",
    "    for idx1, idx2 in top_10_pairs:\n",
    "        tweet1 = train_tweets.iloc[idx1]['text']\n",
    "        tweet2 = train_tweets.iloc[idx2]['text']\n",
    "        print(f\"Tweet indices : ({idx1}, {idx2}), Similarity: {similarity_matrix[idx1, idx2]}\")\n",
    "        print(f\"Tweet 1 : {tweet1}\")\n",
    "        print(f\"Tweet 2: {tweet2}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                    \n",
      "====================================================================================================\n",
      "top 10 similar pair of tweets for the stemming_binary dataset:\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "Tweet indices : (678, 1196), Similarity: 0.8819171036881966\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: get free?,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (490, 678), Similarity: 0.8819171036881966\n",
      "Tweet 1 : happy,thanks recent follow happy connect happy great thursday.,thank much willow!\n",
      "Tweet 2: i,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (160, 1011), Similarity: 0.8819171036881966\n",
      "Tweet 1 : know cute happy,sone cant stream genie!you stream ty genie!\n",
      "Tweet 2: happy,sone cant stream genie!you stream ty genie!\n",
      "--------------------------------------------------\n",
      "Tweet indices : (678, 986), Similarity: 0.8819171036881966\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (77, 678), Similarity: 0.8819171036881966\n",
      "Tweet 1 : good day happy,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: i,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (172, 986), Similarity: 0.8819171036881966\n",
      "Tweet 1 : x,thanks recent follow happy connect happy (want this?\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (678, 1007), Similarity: 0.8819171036881966\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: great story happy,i happy,thanks recent follow happy connect happy great thursday.,maybe?\n",
      "--------------------------------------------------\n",
      "Tweet indices : (446, 1145), Similarity: 0.9258200997725515\n",
      "Tweet 1 : much appreciated happy want ?,thanks recent follow.\n",
      "Tweet 2: much appreciated happy,thanks recent follow.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (1038, 1107), Similarity: 0.9354143466934851\n",
      "Tweet 1 : looking forward reading tweets happy (want this?\n",
      "Tweet 2: looking forward reading tweets happy want this,apr.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (396, 986), Similarity: 0.9486832980505138\n",
      "Tweet 1 : want this,co,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "top 10 similar pair of tweets for the stemming_word_counts dataset:\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "Tweet indices : (678, 986), Similarity: 0.9128709291752769\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (172, 986), Similarity: 0.9128709291752769\n",
      "Tweet 1 : x,thanks recent follow happy connect happy (want this?\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (488, 678), Similarity: 0.9136643851522571\n",
      "Tweet 1 : lots math happy,$es_f $spy bulls relentless happy setups like today,thanks recent follow happy connect happy great thursday.,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: i,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (490, 678), Similarity: 0.9192388155425117\n",
      "Tweet 1 : happy,thanks recent follow happy connect happy great thursday.,thank much willow!\n",
      "Tweet 2: i,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (77, 678), Similarity: 0.9203579866168445\n",
      "Tweet 1 : good day happy,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: i,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (446, 1145), Similarity: 0.9258200997725515\n",
      "Tweet 1 : much appreciated happy want ?,thanks recent follow.\n",
      "Tweet 2: much appreciated happy,thanks recent follow.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (160, 1011), Similarity: 0.9309493362512627\n",
      "Tweet 1 : know cute happy,sone cant stream genie!you stream ty genie!\n",
      "Tweet 2: happy,sone cant stream genie!you stream ty genie!\n",
      "--------------------------------------------------\n",
      "Tweet indices : (1038, 1107), Similarity: 0.9354143466934851\n",
      "Tweet 1 : looking forward reading tweets happy (want this?\n",
      "Tweet 2: looking forward reading tweets happy want this,apr.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (287, 1102), Similarity: 0.9406341620035448\n",
      "Tweet 1 : 1 new follower unfollowers happy via,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: new followers unfollowers happy via,in melbourne please happy,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (396, 986), Similarity: 0.960768922830523\n",
      "Tweet 1 : want this,co,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "top 10 similar pair of tweets for the stemming_tf-idf dataset:\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "Tweet indices : (678, 1196), Similarity: 0.8533857495696598\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: get free?,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (287, 1102), Similarity: 0.8647133440548933\n",
      "Tweet 1 : 1 new follower unfollowers happy via,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: new followers unfollowers happy via,in melbourne please happy,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (99, 280), Similarity: 0.8743911441872524\n",
      "Tweet 1 : happen??\n",
      "Tweet 2: unhappy happen?\n",
      "--------------------------------------------------\n",
      "Tweet indices : (77, 678), Similarity: 0.8825434837176644\n",
      "Tweet 1 : good day happy,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: i,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (172, 986), Similarity: 0.8839819549319872\n",
      "Tweet 1 : x,thanks recent follow happy connect happy (want this?\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (678, 986), Similarity: 0.8980568752532391\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (350, 471), Similarity: 0.9074171386611866\n",
      "Tweet 1 : also epaper.\n",
      "Tweet 2: more, also epaper.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (276, 836), Similarity: 0.908980590185593\n",
      "Tweet 1 : sad ,i miss unhappy ,i absorbed i'm going get laugh gets 2000 votes.\n",
      "Tweet 2: ,i absorbed i'm going get laugh gets 2000 votes.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (446, 1145), Similarity: 0.9385107119538669\n",
      "Tweet 1 : much appreciated happy want ?,thanks recent follow.\n",
      "Tweet 2: much appreciated happy,thanks recent follow.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (160, 1011), Similarity: 0.9413392672151044\n",
      "Tweet 1 : know cute happy,sone cant stream genie!you stream ty genie!\n",
      "Tweet 2: happy,sone cant stream genie!you stream ty genie!\n",
      "--------------------------------------------------\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "top 10 similar pair of tweets for the lemmatization_binary dataset:\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "Tweet indices : (77, 678), Similarity: 0.8819171036881966\n",
      "Tweet 1 : good day happy,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: i,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (172, 986), Similarity: 0.8819171036881966\n",
      "Tweet 1 : x,thanks recent follow happy connect happy (want this?\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (678, 986), Similarity: 0.8819171036881966\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (678, 1196), Similarity: 0.8819171036881966\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: get free?,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (160, 1011), Similarity: 0.8819171036881966\n",
      "Tweet 1 : know cute happy,sone cant stream genie!you stream ty genie!\n",
      "Tweet 2: happy,sone cant stream genie!you stream ty genie!\n",
      "--------------------------------------------------\n",
      "Tweet indices : (678, 1007), Similarity: 0.8819171036881966\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: great story happy,i happy,thanks recent follow happy connect happy great thursday.,maybe?\n",
      "--------------------------------------------------\n",
      "Tweet indices : (287, 1102), Similarity: 0.8864052604279182\n",
      "Tweet 1 : 1 new follower unfollowers happy via,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: new followers unfollowers happy via,in melbourne please happy,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (446, 1145), Similarity: 0.9258200997725515\n",
      "Tweet 1 : much appreciated happy want ?,thanks recent follow.\n",
      "Tweet 2: much appreciated happy,thanks recent follow.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (1038, 1107), Similarity: 0.9354143466934851\n",
      "Tweet 1 : looking forward reading tweets happy (want this?\n",
      "Tweet 2: looking forward reading tweets happy want this,apr.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (396, 986), Similarity: 0.9486832980505138\n",
      "Tweet 1 : want this,co,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "top 10 similar pair of tweets for the lemmatization_word_counts dataset:\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "Tweet indices : (172, 986), Similarity: 0.9128709291752769\n",
      "Tweet 1 : x,thanks recent follow happy connect happy (want this?\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (678, 1196), Similarity: 0.9128709291752769\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: get free?,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (678, 986), Similarity: 0.9128709291752769\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (488, 678), Similarity: 0.9136643851522571\n",
      "Tweet 1 : lots math happy,$es_f $spy bulls relentless happy setups like today,thanks recent follow happy connect happy great thursday.,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: i,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (77, 678), Similarity: 0.9203579866168445\n",
      "Tweet 1 : good day happy,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: i,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (446, 1145), Similarity: 0.9258200997725515\n",
      "Tweet 1 : much appreciated happy want ?,thanks recent follow.\n",
      "Tweet 2: much appreciated happy,thanks recent follow.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (160, 1011), Similarity: 0.9309493362512627\n",
      "Tweet 1 : know cute happy,sone cant stream genie!you stream ty genie!\n",
      "Tweet 2: happy,sone cant stream genie!you stream ty genie!\n",
      "--------------------------------------------------\n",
      "Tweet indices : (1038, 1107), Similarity: 0.9354143466934851\n",
      "Tweet 1 : looking forward reading tweets happy (want this?\n",
      "Tweet 2: looking forward reading tweets happy want this,apr.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (287, 1102), Similarity: 0.937231510253071\n",
      "Tweet 1 : 1 new follower unfollowers happy via,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: new followers unfollowers happy via,in melbourne please happy,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (396, 986), Similarity: 0.960768922830523\n",
      "Tweet 1 : want this,co,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "top 10 similar pair of tweets for the lemmatization_tf-idf dataset:\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "Tweet indices : (678, 1196), Similarity: 0.8620816008176059\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: get free?,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (287, 1102), Similarity: 0.8633876078509463\n",
      "Tweet 1 : 1 new follower unfollowers happy via,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: new followers unfollowers happy via,in melbourne please happy,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (99, 280), Similarity: 0.8764314302062407\n",
      "Tweet 1 : happen??\n",
      "Tweet 2: unhappy happen?\n",
      "--------------------------------------------------\n",
      "Tweet indices : (172, 986), Similarity: 0.8864182454927169\n",
      "Tweet 1 : x,thanks recent follow happy connect happy (want this?\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (77, 678), Similarity: 0.8873484341439979\n",
      "Tweet 1 : good day happy,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: i,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (678, 986), Similarity: 0.9006041609684381\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (276, 836), Similarity: 0.9054159117506817\n",
      "Tweet 1 : sad ,i miss unhappy ,i absorbed i'm going get laugh gets 2000 votes.\n",
      "Tweet 2: ,i absorbed i'm going get laugh gets 2000 votes.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (350, 471), Similarity: 0.9074171386611866\n",
      "Tweet 1 : also epaper.\n",
      "Tweet 2: more, also epaper.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (446, 1145), Similarity: 0.9402217684919763\n",
      "Tweet 1 : much appreciated happy want ?,thanks recent follow.\n",
      "Tweet 2: much appreciated happy,thanks recent follow.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (160, 1011), Similarity: 0.9417845528920301\n",
      "Tweet 1 : know cute happy,sone cant stream genie!you stream ty genie!\n",
      "Tweet 2: happy,sone cant stream genie!you stream ty genie!\n",
      "--------------------------------------------------\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "top 10 similar pair of tweets for the stemming_misspelling_correction_binary dataset:\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "Tweet indices : (678, 1196), Similarity: 0.8819171036881966\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: get free?,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (490, 678), Similarity: 0.8819171036881966\n",
      "Tweet 1 : happy,thanks recent follow happy connect happy great thursday.,thank much willow!\n",
      "Tweet 2: i,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (160, 1011), Similarity: 0.8819171036881966\n",
      "Tweet 1 : know cute happy,sone cant stream genie!you stream ty genie!\n",
      "Tweet 2: happy,sone cant stream genie!you stream ty genie!\n",
      "--------------------------------------------------\n",
      "Tweet indices : (678, 986), Similarity: 0.8819171036881966\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (77, 678), Similarity: 0.8819171036881966\n",
      "Tweet 1 : good day happy,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: i,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (172, 986), Similarity: 0.8819171036881966\n",
      "Tweet 1 : x,thanks recent follow happy connect happy (want this?\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (678, 1007), Similarity: 0.8819171036881966\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: great story happy,i happy,thanks recent follow happy connect happy great thursday.,maybe?\n",
      "--------------------------------------------------\n",
      "Tweet indices : (446, 1145), Similarity: 0.9258200997725515\n",
      "Tweet 1 : much appreciated happy want ?,thanks recent follow.\n",
      "Tweet 2: much appreciated happy,thanks recent follow.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (1038, 1107), Similarity: 0.9354143466934851\n",
      "Tweet 1 : looking forward reading tweets happy (want this?\n",
      "Tweet 2: looking forward reading tweets happy want this,apr.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (396, 986), Similarity: 0.9486832980505138\n",
      "Tweet 1 : want this,co,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "top 10 similar pair of tweets for the stemming_misspelling_correction_word_counts dataset:\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "Tweet indices : (678, 986), Similarity: 0.9128709291752769\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (172, 986), Similarity: 0.9128709291752769\n",
      "Tweet 1 : x,thanks recent follow happy connect happy (want this?\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (488, 678), Similarity: 0.9136643851522571\n",
      "Tweet 1 : lots math happy,$es_f $spy bulls relentless happy setups like today,thanks recent follow happy connect happy great thursday.,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: i,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (490, 678), Similarity: 0.9192388155425117\n",
      "Tweet 1 : happy,thanks recent follow happy connect happy great thursday.,thank much willow!\n",
      "Tweet 2: i,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (77, 678), Similarity: 0.9203579866168445\n",
      "Tweet 1 : good day happy,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: i,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (446, 1145), Similarity: 0.9258200997725515\n",
      "Tweet 1 : much appreciated happy want ?,thanks recent follow.\n",
      "Tweet 2: much appreciated happy,thanks recent follow.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (160, 1011), Similarity: 0.9309493362512627\n",
      "Tweet 1 : know cute happy,sone cant stream genie!you stream ty genie!\n",
      "Tweet 2: happy,sone cant stream genie!you stream ty genie!\n",
      "--------------------------------------------------\n",
      "Tweet indices : (1038, 1107), Similarity: 0.9354143466934851\n",
      "Tweet 1 : looking forward reading tweets happy (want this?\n",
      "Tweet 2: looking forward reading tweets happy want this,apr.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (287, 1102), Similarity: 0.9406341620035448\n",
      "Tweet 1 : 1 new follower unfollowers happy via,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: new followers unfollowers happy via,in melbourne please happy,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (396, 986), Similarity: 0.960768922830523\n",
      "Tweet 1 : want this,co,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "top 10 similar pair of tweets for the stemming_misspelling_correction_tf-idf dataset:\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "Tweet indices : (678, 1196), Similarity: 0.8533857495696598\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: get free?,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (287, 1102), Similarity: 0.8647133440548933\n",
      "Tweet 1 : 1 new follower unfollowers happy via,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: new followers unfollowers happy via,in melbourne please happy,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (99, 280), Similarity: 0.8743911441872524\n",
      "Tweet 1 : happen??\n",
      "Tweet 2: unhappy happen?\n",
      "--------------------------------------------------\n",
      "Tweet indices : (77, 678), Similarity: 0.8825434837176644\n",
      "Tweet 1 : good day happy,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: i,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (172, 986), Similarity: 0.8839819549319872\n",
      "Tweet 1 : x,thanks recent follow happy connect happy (want this?\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (678, 986), Similarity: 0.8980568752532391\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (350, 471), Similarity: 0.9074171386611866\n",
      "Tweet 1 : also epaper.\n",
      "Tweet 2: more, also epaper.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (276, 836), Similarity: 0.908980590185593\n",
      "Tweet 1 : sad ,i miss unhappy ,i absorbed i'm going get laugh gets 2000 votes.\n",
      "Tweet 2: ,i absorbed i'm going get laugh gets 2000 votes.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (446, 1145), Similarity: 0.9385107119538669\n",
      "Tweet 1 : much appreciated happy want ?,thanks recent follow.\n",
      "Tweet 2: much appreciated happy,thanks recent follow.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (160, 1011), Similarity: 0.9413392672151044\n",
      "Tweet 1 : know cute happy,sone cant stream genie!you stream ty genie!\n",
      "Tweet 2: happy,sone cant stream genie!you stream ty genie!\n",
      "--------------------------------------------------\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "top 10 similar pair of tweets for the lemmatization_misspelling_correction_binary dataset:\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "Tweet indices : (77, 678), Similarity: 0.8819171036881966\n",
      "Tweet 1 : good day happy,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: i,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (172, 986), Similarity: 0.8819171036881966\n",
      "Tweet 1 : x,thanks recent follow happy connect happy (want this?\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (678, 986), Similarity: 0.8819171036881966\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (678, 1196), Similarity: 0.8819171036881966\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: get free?,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (160, 1011), Similarity: 0.8819171036881966\n",
      "Tweet 1 : know cute happy,sone cant stream genie!you stream ty genie!\n",
      "Tweet 2: happy,sone cant stream genie!you stream ty genie!\n",
      "--------------------------------------------------\n",
      "Tweet indices : (678, 1007), Similarity: 0.8819171036881966\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: great story happy,i happy,thanks recent follow happy connect happy great thursday.,maybe?\n",
      "--------------------------------------------------\n",
      "Tweet indices : (287, 1102), Similarity: 0.8864052604279182\n",
      "Tweet 1 : 1 new follower unfollowers happy via,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: new followers unfollowers happy via,in melbourne please happy,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (446, 1145), Similarity: 0.9258200997725515\n",
      "Tweet 1 : much appreciated happy want ?,thanks recent follow.\n",
      "Tweet 2: much appreciated happy,thanks recent follow.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (1038, 1107), Similarity: 0.9354143466934851\n",
      "Tweet 1 : looking forward reading tweets happy (want this?\n",
      "Tweet 2: looking forward reading tweets happy want this,apr.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (396, 986), Similarity: 0.9486832980505138\n",
      "Tweet 1 : want this,co,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "top 10 similar pair of tweets for the lemmatization_misspelling_correction_word_counts dataset:\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "Tweet indices : (172, 986), Similarity: 0.9128709291752769\n",
      "Tweet 1 : x,thanks recent follow happy connect happy (want this?\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (678, 1196), Similarity: 0.9128709291752769\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: get free?,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (678, 986), Similarity: 0.9128709291752769\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (488, 678), Similarity: 0.9136643851522571\n",
      "Tweet 1 : lots math happy,$es_f $spy bulls relentless happy setups like today,thanks recent follow happy connect happy great thursday.,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: i,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (77, 678), Similarity: 0.9203579866168445\n",
      "Tweet 1 : good day happy,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: i,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (446, 1145), Similarity: 0.9258200997725515\n",
      "Tweet 1 : much appreciated happy want ?,thanks recent follow.\n",
      "Tweet 2: much appreciated happy,thanks recent follow.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (160, 1011), Similarity: 0.9309493362512627\n",
      "Tweet 1 : know cute happy,sone cant stream genie!you stream ty genie!\n",
      "Tweet 2: happy,sone cant stream genie!you stream ty genie!\n",
      "--------------------------------------------------\n",
      "Tweet indices : (1038, 1107), Similarity: 0.9354143466934851\n",
      "Tweet 1 : looking forward reading tweets happy (want this?\n",
      "Tweet 2: looking forward reading tweets happy want this,apr.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (287, 1102), Similarity: 0.937231510253071\n",
      "Tweet 1 : 1 new follower unfollowers happy via,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: new followers unfollowers happy via,in melbourne please happy,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (396, 986), Similarity: 0.960768922830523\n",
      "Tweet 1 : want this,co,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "top 10 similar pair of tweets for the lemmatization_misspelling_correction_tf-idf dataset:\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "Tweet indices : (678, 1196), Similarity: 0.8620816008176059\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: get free?,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (287, 1102), Similarity: 0.8633876078509463\n",
      "Tweet 1 : 1 new follower unfollowers happy via,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: new followers unfollowers happy via,in melbourne please happy,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (99, 280), Similarity: 0.8764314302062407\n",
      "Tweet 1 : happen??\n",
      "Tweet 2: unhappy happen?\n",
      "--------------------------------------------------\n",
      "Tweet indices : (172, 986), Similarity: 0.8864182454927169\n",
      "Tweet 1 : x,thanks recent follow happy connect happy (want this?\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (77, 678), Similarity: 0.8873484341439979\n",
      "Tweet 1 : good day happy,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: i,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (678, 986), Similarity: 0.9006041609684381\n",
      "Tweet 1 : i,thanks recent follow happy connect happy great thursday.\n",
      "Tweet 2: want this,thanks recent follow happy connect happy great thursday.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (276, 836), Similarity: 0.9054159117506817\n",
      "Tweet 1 : sad ,i miss unhappy ,i absorbed i'm going get laugh gets 2000 votes.\n",
      "Tweet 2: ,i absorbed i'm going get laugh gets 2000 votes.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (350, 471), Similarity: 0.9074171386611866\n",
      "Tweet 1 : also epaper.\n",
      "Tweet 2: more, also epaper.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (446, 1145), Similarity: 0.9402217684919763\n",
      "Tweet 1 : much appreciated happy want ?,thanks recent follow.\n",
      "Tweet 2: much appreciated happy,thanks recent follow.\n",
      "--------------------------------------------------\n",
      "Tweet indices : (160, 1011), Similarity: 0.9417845528920301\n",
      "Tweet 1 : know cute happy,sone cant stream genie!you stream ty genie!\n",
      "Tweet 2: happy,sone cant stream genie!you stream ty genie!\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "top_similar_tweets = {}\n",
    "for dataset_name, (vectorized_data, labels) in datasets.items():\n",
    "    if dataset_name.endswith('_test'):\n",
    "        continue\n",
    "    print(\" \" * 100)\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"top 10 similar pair of tweets for the {dataset_name} dataset:\")\n",
    "    print(\"=\" * 100)\n",
    "    print(\" \" * 100)\n",
    "    top_similar_tweets[dataset_name] = find_top_n_similar_tweets(vectorized_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentiment analysis\n",
    "now i can use some of the different datasets that i prepared for sentiment analysis using different algorithms \n",
    "\n",
    "datasets used for the sentiment analysis task : \n",
    "- stemming_binary \n",
    "- lemmatization_tfidf\n",
    "- stemming_misspelling_correction_word_counts\n",
    "- lemmatization_misspelling_correction_binary\n",
    "- lemmatization_misspelling_correction_tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "## first i'll start by defining a function that saves the model and a function that loads the model\n",
    "\n",
    "def save_model(model, folder_name, file_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    \n",
    "    # save the model\n",
    "    model_path = os.path.join(folder_name, file_name)\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "def load_model(folder_name, file_name):\n",
    "    model_path = os.path.join(folder_name, file_name)\n",
    "    model = joblib.load(model_path)\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the datasets that i want to use\n",
    "selected_datasets = [\n",
    "    \"stemming_binary\",\n",
    "    \"lemmatization_tf-idf\",\n",
    "    \"stemming_misspelling_correction_word_counts\", \n",
    "    \"lemmatization_misspelling_correction_binary\", \n",
    "    \"lemmatization_misspelling_correction_tf-idf\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis using Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dict to store the metrics for Logistic regression model across different datasets\n",
    "Logistic_regression_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to lr_models/lr_stemming_binary_model.joblib\n",
      "Model saved to lr_models/lr_lemmatization_tf-idf_model.joblib\n",
      "Model saved to lr_models/lr_stemming_misspelling_correction_word_counts_model.joblib\n",
      "Model saved to lr_models/lr_lemmatization_misspelling_correction_binary_model.joblib\n",
      "Model saved to lr_models/lr_lemmatization_misspelling_correction_tf-idf_model.joblib\n",
      "\n",
      "Metrics for stemming_binary:\n",
      "Accuracy: 0.8958\n",
      "\n",
      "Metrics for lemmatization_tf-idf:\n",
      "Accuracy: 0.8827\n",
      "\n",
      "Metrics for stemming_misspelling_correction_word_counts:\n",
      "Accuracy: 0.8958\n",
      "\n",
      "Metrics for lemmatization_misspelling_correction_binary:\n",
      "Accuracy: 0.8925\n",
      "\n",
      "Metrics for lemmatization_misspelling_correction_tf-idf:\n",
      "Accuracy: 0.8827\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in selected_datasets:\n",
    "    # load train and test data\n",
    "    X_train, y_train = datasets[dataset_name]\n",
    "    X_test, y_test = datasets[dataset_name + \"_test\"]\n",
    "\n",
    "    lr_model = LogisticRegression(max_iter=1000)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = lr_model.predict(X_test)\n",
    "\n",
    "    # calculate accuracy metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    Logistic_regression_metrics[dataset_name] = {\n",
    "        \"accuracy\": accuracy\n",
    "    }\n",
    "\n",
    "    save_model(lr_model, 'lr_models', f\"lr_{dataset_name}_model.joblib\")\n",
    "\n",
    "# print results\n",
    "for dataset_name, metrics in Logistic_regression_metrics.items():\n",
    "    print(f\"\\nMetrics for {dataset_name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dict to store the metrics for SVM model across different datasets\n",
    "SVM_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to svm_models/svm_stemming_binary_model.joblib\n",
      "Model saved to svm_models/svm_lemmatization_tf-idf_model.joblib\n",
      "Model saved to svm_models/svm_stemming_misspelling_correction_word_counts_model.joblib\n",
      "Model saved to svm_models/svm_lemmatization_misspelling_correction_binary_model.joblib\n",
      "Model saved to svm_models/svm_lemmatization_misspelling_correction_tf-idf_model.joblib\n",
      "\n",
      "Metrics for stemming_binary:\n",
      "Accuracy: 0.8893\n",
      "\n",
      "Metrics for lemmatization_tf-idf:\n",
      "Accuracy: 0.9088\n",
      "\n",
      "Metrics for stemming_misspelling_correction_word_counts:\n",
      "Accuracy: 0.8925\n",
      "\n",
      "Metrics for lemmatization_misspelling_correction_binary:\n",
      "Accuracy: 0.8827\n",
      "\n",
      "Metrics for lemmatization_misspelling_correction_tf-idf:\n",
      "Accuracy: 0.9088\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in selected_datasets:\n",
    "    # load train and test data\n",
    "    X_train, y_train = datasets[dataset_name]\n",
    "    X_test, y_test = datasets[dataset_name + \"_test\"]\n",
    "\n",
    "    # train the SVM model\n",
    "    svm_model = SVC(kernel='linear')\n",
    "    svm_model.fit(X_train, y_train)\n",
    "\n",
    "    # predicting with SVM\n",
    "    svm_predictions = svm_model.predict(X_test)\n",
    "\n",
    "    # calculat model metrics\n",
    "    accuracy = accuracy_score(y_test, svm_predictions)\n",
    "\n",
    "    SVM_metrics[dataset_name] = {\n",
    "        \"accuracy\" : accuracy\n",
    "    }\n",
    "\n",
    "    save_model(svm_model, 'svm_models', f\"svm_{dataset_name}_model.joblib\")\n",
    "\n",
    "    \n",
    "# print results\n",
    "for dataset_name, metrics in SVM_metrics.items():\n",
    "    print(f\"\\nMetrics for {dataset_name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis using random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dict to store the metrics for random forest model across different datasets\n",
    "random_forest_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to rf_models/rf_stemming_binary_model.joblib\n",
      "Model saved to rf_models/rf_lemmatization_tf-idf_model.joblib\n",
      "Model saved to rf_models/rf_stemming_misspelling_correction_word_counts_model.joblib\n",
      "Model saved to rf_models/rf_lemmatization_misspelling_correction_binary_model.joblib\n",
      "Model saved to rf_models/rf_lemmatization_misspelling_correction_tf-idf_model.joblib\n",
      "\n",
      "Metrics for stemming_binary:\n",
      "Accuracy: 0.8697\n",
      "\n",
      "Metrics for lemmatization_tf-idf:\n",
      "Accuracy: 0.8599\n",
      "\n",
      "Metrics for stemming_misspelling_correction_word_counts:\n",
      "Accuracy: 0.8567\n",
      "\n",
      "Metrics for lemmatization_misspelling_correction_binary:\n",
      "Accuracy: 0.8567\n",
      "\n",
      "Metrics for lemmatization_misspelling_correction_tf-idf:\n",
      "Accuracy: 0.8599\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in selected_datasets:\n",
    "    # load train and test data\n",
    "    X_train, y_train = datasets[dataset_name]\n",
    "    X_test, y_test = datasets[dataset_name + \"_test\"]\n",
    "\n",
    "    # train the random forest model\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # predicting with random Forest\n",
    "    rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "    # calculat model metrics\n",
    "    accuracy = accuracy_score(y_test, rf_predictions)\n",
    "\n",
    "    random_forest_metrics[dataset_name] = {\n",
    "        \"accuracy\" : accuracy\n",
    "    }\n",
    "\n",
    "    save_model(rf_model, 'rf_models', f\"rf_{dataset_name}_model.joblib\")\n",
    "\n",
    "    \n",
    "# print results\n",
    "for dataset_name, metrics in random_forest_metrics.items():\n",
    "    print(f\"\\nMetrics for {dataset_name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluating the model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from svm_models/svm_lemmatization_misspelling_correction_binary_model.joblib\n",
      "1\n",
      "The predicted sentiment is: Positive\n"
     ]
    }
   ],
   "source": [
    "label_mapping = {1: \"Positive\", 0: \"Neutral\", -1: \"Negative\"}\n",
    "\n",
    "def preprocess_and_vectorize(text, vectorizer):\n",
    "    vectorized_text = vectorizer.transform([text])\n",
    "    return vectorized_text\n",
    "\n",
    "# defining a function to predict a sentiment\n",
    "def predict_sentiment(text, load_model, vectorizer, label_mapping):\n",
    "    # Preprocess and vectorize input text\n",
    "    vectorized_text = preprocess_and_vectorize(text, vectorizer)\n",
    "    \n",
    "    # Predict the sentiment\n",
    "    prediction = load_model.predict(vectorized_text)[0]\n",
    "    print(prediction)\n",
    "    \n",
    "    # Map the prediction to the sentiment label\n",
    "    sentiment = label_mapping.get(prediction, \"Unknown\")\n",
    "    return sentiment\n",
    "\n",
    "positive_sample_text = \"The children joyfully play in the sunny park while their parents smile. One child loves running and catching butterflies, feeling free and happy under the bright sky.\"\n",
    "negative_sample_text = \"The children sit alone in the gloomy park, looking bored and unhappy. One child sighs, saying he hates being stuck inside, while another complains about not having anyone to play with.\"\n",
    "neutral_sample_text = \"The children are playing in the park while their parents watch. One child talks about running and catching butterflies, while another is sitting and observing the surroundings.\"\n",
    "preprocessing_func = preprocessing_methods['lemmatization_misspelling_correction']\n",
    "preprocessed_text = preprocessing_func(positive_sample_text)\n",
    "vectorizer = vectorizers['lemmatization_misspelling_correction_binary']\n",
    "loaded_model = load_model('svm_models', f\"svm_lemmatization_misspelling_correction_binary_model.joblib\")\n",
    "# loaded_model = load_model('lr_models', f\"lr_lemmatization_misspelling_correction_binary_model.joblib\")\n",
    "# loaded_model = load_model('rf_models', f\"rf_lemmatization_misspelling_correction_binary_model.joblib\")\n",
    "predicted_sentiment = predict_sentiment(positive_sample_text, loaded_model, vectorizer, label_mapping)\n",
    "print(f\"The predicted sentiment is: {predicted_sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
