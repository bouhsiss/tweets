{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading datasets\n",
    "i'll start by loading the datasets into pandas dataframes, and then i'll perform some basic cleaning and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# define the path to the data\n",
    "path_to_data = '../data/raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  sentiment\n",
      "0  An inspiration in all aspects: Fashion, fitnes...          1\n",
      "1  :)KISSES TheFashionIcon,Apka Apna Awam Ka Chan...          1\n",
      "2  Can you donate?,Omg he... kissed... him crying...          1\n",
      "3  love love happy,thanks happy,C'mon Tweeps, Joi...          1\n",
      "4                                Do spread the word.          1\n",
      "(587, 2)\n",
      "                                                text  sentiment\n",
      "0  How unhappy  some dogs like it though,talking ...         -1\n",
      "1  I got some money  I need to change into R but ...         -1\n",
      "2  unhappy ,it's that A*dy guy from pop Asia and ...         -1\n",
      "3                            Is this how I find out.         -1\n",
      "4                                Everyone knows now.         -1\n",
      "(357, 2)\n",
      "                                                text  sentiment\n",
      "0  Pak PM survives removal scare, but court order...          0\n",
      "1  ,Supreme Court quashes criminal complaint agai...          0\n",
      "2  ,FCRA slap on NGO for lobbying...But was it do...          0\n",
      "3  ,Why doctors, pharma companies are opposing na...          0\n",
      "4  His officer learnt ground reality -- and  a di...          0\n",
      "(684, 2)\n"
     ]
    }
   ],
   "source": [
    "# read the csv files content\n",
    "with open(os.path.join(path_to_data, 'processedPositive.csv'), 'r') as file:\n",
    "    happy_content = file.read()\n",
    "with open(os.path.join(path_to_data, 'processedNegative.csv'), 'r') as file:\n",
    "    sad_content = file.read()\n",
    "with open(os.path.join(path_to_data, 'processedNeutral.csv'), 'r') as file:\n",
    "    neutral_content = file.read()\n",
    "\n",
    "# split the content into individual tweets\n",
    "happy_sentences = sent_tokenize(happy_content)\n",
    "happy_tweets = pd.DataFrame(happy_sentences, columns=['text'])\n",
    "happy_tweets['sentiment'] = 1\n",
    "sad_sentences = sent_tokenize(sad_content)\n",
    "sad_tweets = pd.DataFrame(sad_sentences, columns=['text'])\n",
    "sad_tweets['sentiment'] = -1\n",
    "neutral_sentences = sent_tokenize(neutral_content)\n",
    "neutral_tweets = pd.DataFrame(neutral_sentences, columns=['text'])\n",
    "neutral_tweets['sentiment'] = 0\n",
    "\n",
    "print(happy_tweets.head())\n",
    "print(happy_tweets.shape)\n",
    "print(sad_tweets.head())\n",
    "print(sad_tweets.shape)\n",
    "print(neutral_tweets.head())\n",
    "print(neutral_tweets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  merge and clean the data\n",
    "in this step we'll clean the data by removing the duplicates and stop words, which are often meaningless words that can add noise to the dataset rather than meaningful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  sentiment\n",
      "0  An inspiration in all aspects: Fashion, fitnes...          1\n",
      "1  :)KISSES TheFashionIcon,Apka Apna Awam Ka Chan...          1\n",
      "2  Can you donate?,Omg he... kissed... him crying...          1\n",
      "3  love love happy,thanks happy,C'mon Tweeps, Joi...          1\n",
      "4                                Do spread the word.          1\n",
      "(1628, 2)\n"
     ]
    }
   ],
   "source": [
    "merged_tweets = pd.concat([happy_tweets, sad_tweets, neutral_tweets], ignore_index=True)\n",
    "merged_tweets.dropna(inplace=True)\n",
    "merged_tweets.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(merged_tweets.head())\n",
    "print(merged_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shrooma/miniconda3/envs/tweets/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  sentiment\n",
      "0  inspiration aspects: fashion, fitness, beauty ...          1\n",
      "1  :)kisses thefashionicon,apka apna awam ka chan...          1\n",
      "2  donate?,omg he... kissed... crying joy,happy a...          1\n",
      "3  love love happy,thanks happy,c'mon tweeps, joi...          1\n",
      "4                                       spread word.          1\n",
      "(1533, 2)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# transform the text to lowercase\n",
    "merged_tweets['text'] = merged_tweets['text'].str.lower()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# remove stop words\n",
    "merged_tweets['text'] = merged_tweets['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords.words('english')]))\n",
    "\n",
    "# remove duplicates\n",
    "merged_tweets.drop_duplicates(subset=['text'], inplace=True)\n",
    "merged_tweets.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(merged_tweets.head())\n",
    "print(merged_tweets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split the data with stratification\n",
    "making a split on the train and test (20%) datasets with stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1226, 2)\n",
      "(307, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_tweets, test_tweets = train_test_split(merged_tweets,\n",
    "                                             test_size=0.2,\n",
    "                                             stratify=merged_tweets['sentiment'],\n",
    "                                             random_state=42)\n",
    "\n",
    "print(train_tweets.shape)\n",
    "print(test_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset:\n",
      "sentiment\n",
      " 0    0.397913\n",
      " 1    0.371168\n",
      "-1    0.230920\n",
      "Name: proportion, dtype: float64\n",
      "Train dataset:\n",
      "sentiment\n",
      " 0    0.398042\n",
      " 1    0.371126\n",
      "-1    0.230832\n",
      "Name: proportion, dtype: float64\n",
      "Test dataset:\n",
      "sentiment\n",
      " 0    0.397394\n",
      " 1    0.371336\n",
      "-1    0.231270\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# check the distribution of the sentiment in the original, train and test datasets\n",
    "print(\"Original dataset:\")\n",
    "print(merged_tweets['sentiment'].value_counts(normalize=True))\n",
    "print(\"Train dataset:\")\n",
    "print(train_tweets['sentiment'].value_counts(normalize=True))\n",
    "print(\"Test dataset:\")\n",
    "print(test_tweets['sentiment'].value_counts(normalize=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess and vectorize the data\n",
    "\n",
    "in this step i'll prepare multiple datasets, each dataset is a combination of a preprocessing approach and a vectorization approach.\n",
    "\n",
    "-- preprocessing approaches :\n",
    "- text tokenization\n",
    "- stemming\n",
    "- lemmatization\n",
    "- stemming + mispellings correction\n",
    "- lemmatization + mispellings correction\n",
    "\n",
    "-- vectorization approaches :\n",
    "- binary vectorization\n",
    "- word counts\n",
    "- tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# import the preprocessing and vectorization functions\n",
    "from utils.text_preprocessing import tokenize, stem_text, lemmatize_text, correct_spelling\n",
    "from utils.text_vectorization import binary_vectorizer, count_vectorizer, tfidf_vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionnary to store the datasets\n",
    "datasets = {}\n",
    "\n",
    "# preprocessing and vectorization dictionary\n",
    "preprocessing_methods = {\n",
    "    'stemming' : stem_text,\n",
    "    'lemmatization' : lemmatize_text,\n",
    "    'stemming_misspelling_correction' : lambda x: correct_spelling(stem_text(x)),\n",
    "    'lemmatization_misspelling_correction' : lambda x: correct_spelling(lemmatize_text(x))\n",
    "}\n",
    "\n",
    "vectorization_methods = {\n",
    "    'binary' : binary_vectorizer,\n",
    "    'word_counts' : count_vectorizer,\n",
    "    'tf-idf' : tfidf_vectorizer\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['stemming_binary', 'stemming_binary_test', 'stemming_word_counts', 'stemming_word_counts_test', 'stemming_tf-idf', 'stemming_tf-idf_test', 'lemmatization_binary', 'lemmatization_binary_test', 'lemmatization_word_counts', 'lemmatization_word_counts_test', 'lemmatization_tf-idf', 'lemmatization_tf-idf_test', 'stemming_misspelling_correction_binary', 'stemming_misspelling_correction_binary_test', 'stemming_misspelling_correction_word_counts', 'stemming_misspelling_correction_word_counts_test', 'stemming_misspelling_correction_tf-idf', 'stemming_misspelling_correction_tf-idf_test', 'lemmatization_misspelling_correction_binary', 'lemmatization_misspelling_correction_binary_test', 'lemmatization_misspelling_correction_word_counts', 'lemmatization_misspelling_correction_word_counts_test', 'lemmatization_misspelling_correction_tf-idf', 'lemmatization_misspelling_correction_tf-idf_test'])\n"
     ]
    }
   ],
   "source": [
    "for prep_name, prep_func in preprocessing_methods.items():\n",
    "    # preprocess the train and test data\n",
    "    train_preprocessed = train_tweets['text'].apply(prep_func)\n",
    "    test_preprocessed = test_tweets['text'].apply(prep_func)\n",
    "\n",
    "    for vec_name, vec_fund in vectorization_methods.items():\n",
    "        train_vectorized, vectorizer = vec_fund(train_preprocessed)\n",
    "        test_vectorized = vectorizer.transform(test_preprocessed)\n",
    "\n",
    "        # store the datasets with labels\n",
    "        datasets[f'{prep_name}_{vec_name}'] = (train_vectorized, train_tweets['sentiment'])\n",
    "        datasets[f'{prep_name}_{vec_name}_test'] = (test_vectorized, test_tweets['sentiment'])\n",
    "\n",
    "\n",
    "print(datasets.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
