{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading datasets\n",
    "i'll start by loading the datasets into pandas dataframes, and then i'll perform some basic cleaning and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "# define the path to the data\n",
    "path_to_data = '../data/raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  sentiment\n",
      "0             An inspiration in all aspects: Fashion          1\n",
      "1                                            fitness          1\n",
      "2    beauty and personality. :)KISSES TheFashionIcon          1\n",
      "3  Apka Apna Awam Ka Channel Frankline Tv Aam Adm...          1\n",
      "4  Beautiful album from  the greatest unsung guit...          1\n",
      "(1186, 2)\n",
      "                                                text  sentiment\n",
      "0              How unhappy  some dogs like it though         -1\n",
      "1  talking to my over driver about where I'm goin...         -1\n",
      "2  Does anybody know if the Rand's likely to fall...         -1\n",
      "3         I miss going to gigs in Liverpool unhappy          -1\n",
      "4      There isnt a new Riverdale tonight ? unhappy          -1\n",
      "(1117, 2)\n",
      "                                                text  sentiment\n",
      "0                      Pak PM survives removal scare          0\n",
      "1   but court orders further probe into corruptio...          0\n",
      "2  Supreme Court quashes criminal complaint again...          0\n",
      "3  Art of Living's fights back over Yamuna floodp...          0\n",
      "4                                            livid.           0\n",
      "(1570, 2)\n"
     ]
    }
   ],
   "source": [
    "# read the csv files content\n",
    "with open(os.path.join(path_to_data, 'processedPositive.csv'), 'r') as file:\n",
    "    happy_content = file.read()\n",
    "with open(os.path.join(path_to_data, 'processedNegative.csv'), 'r') as file:\n",
    "    sad_content = file.read()\n",
    "with open(os.path.join(path_to_data, 'processedNeutral.csv'), 'r') as file:\n",
    "    neutral_content = file.read()\n",
    "\n",
    "# split the content into individual tweets\n",
    "happy_tweets = pd.DataFrame(happy_content.split(','), columns=['text'])\n",
    "happy_tweets['sentiment'] = 1\n",
    "sad_tweets = pd.DataFrame(sad_content.split(','), columns=['text'])\n",
    "sad_tweets['sentiment'] = -1\n",
    "neutral_tweets = pd.DataFrame(neutral_content.split(','), columns=['text'])\n",
    "neutral_tweets['sentiment'] = 0\n",
    "\n",
    "print(happy_tweets.head())\n",
    "print(happy_tweets.shape)\n",
    "print(sad_tweets.head())\n",
    "print(sad_tweets.shape)\n",
    "print(neutral_tweets.head())\n",
    "print(neutral_tweets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  merge and clean the data\n",
    "in this step we'll clean the data by removing the duplicates and stop words, which are often meaningless words that can add noise to the dataset rather than meaningful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  sentiment\n",
      "0             An inspiration in all aspects: Fashion          1\n",
      "1                                            fitness          1\n",
      "2    beauty and personality. :)KISSES TheFashionIcon          1\n",
      "3  Apka Apna Awam Ka Channel Frankline Tv Aam Adm...          1\n",
      "4  Beautiful album from  the greatest unsung guit...          1\n",
      "(3873, 2)\n"
     ]
    }
   ],
   "source": [
    "merged_tweets = pd.concat([happy_tweets, sad_tweets, neutral_tweets], ignore_index=True)\n",
    "merged_tweets.dropna(inplace=True)\n",
    "merged_tweets.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(merged_tweets.head())\n",
    "print(merged_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shrooma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  sentiment\n",
      "0                       inspiration aspects: fashion          1\n",
      "1                                            fitness          1\n",
      "2        beauty personality. :)kisses thefashionicon          1\n",
      "3  apka apna awam ka channel frankline tv aam adm...          1\n",
      "4  beautiful album greatest unsung guitar genius ...          1\n",
      "(3395, 2)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# transform the text to lowercase\n",
    "merged_tweets['text'] = merged_tweets['text'].str.lower()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# remove stop words\n",
    "merged_tweets['text'] = merged_tweets['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords.words('english')]))\n",
    "\n",
    "# remove duplicates\n",
    "merged_tweets.drop_duplicates(subset=['text'], inplace=True)\n",
    "merged_tweets.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(merged_tweets.head())\n",
    "print(merged_tweets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split the data with stratification\n",
    "making a split on the train and test (20%) datasets with stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2716, 2)\n",
      "(679, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_tweets, test_tweets = train_test_split(merged_tweets,\n",
    "                                             test_size=0.2,\n",
    "                                             stratify=merged_tweets['sentiment'],\n",
    "                                             random_state=42)\n",
    "\n",
    "print(train_tweets.shape)\n",
    "print(test_tweets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess and vectorize the data\n",
    "\n",
    "in this step i'll prepare multiple datasets, each dataset is a combination of a preprocessing approach and a vectorization approach.\n",
    "\n",
    "preprocessing approaches : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
