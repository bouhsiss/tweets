{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading datasets\n",
    "i'll start by loading the datasets into pandas dataframes, and then i'll perform some basic cleaning and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# define the path to the data\n",
    "path_to_data = '../data/raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  sentiment\n",
      "0  An inspiration in all aspects: Fashion, fitnes...          1\n",
      "1  :)KISSES TheFashionIcon,Apka Apna Awam Ka Chan...          1\n",
      "2  Can you donate?,Omg he... kissed... him crying...          1\n",
      "3  love love happy,thanks happy,C'mon Tweeps, Joi...          1\n",
      "4                                Do spread the word.          1\n",
      "(587, 2)\n",
      "                                                text  sentiment\n",
      "0  How unhappy  some dogs like it though,talking ...         -1\n",
      "1  I got some money  I need to change into R but ...         -1\n",
      "2  unhappy ,it's that A*dy guy from pop Asia and ...         -1\n",
      "3                            Is this how I find out.         -1\n",
      "4                                Everyone knows now.         -1\n",
      "(357, 2)\n",
      "                                                text  sentiment\n",
      "0  Pak PM survives removal scare, but court order...          0\n",
      "1  ,Supreme Court quashes criminal complaint agai...          0\n",
      "2  ,FCRA slap on NGO for lobbying...But was it do...          0\n",
      "3  ,Why doctors, pharma companies are opposing na...          0\n",
      "4  His officer learnt ground reality -- and  a di...          0\n",
      "(684, 2)\n"
     ]
    }
   ],
   "source": [
    "# read the csv files content\n",
    "with open(os.path.join(path_to_data, 'processedPositive.csv'), 'r') as file:\n",
    "    happy_content = file.read()\n",
    "with open(os.path.join(path_to_data, 'processedNegative.csv'), 'r') as file:\n",
    "    sad_content = file.read()\n",
    "with open(os.path.join(path_to_data, 'processedNeutral.csv'), 'r') as file:\n",
    "    neutral_content = file.read()\n",
    "\n",
    "# split the content into individual tweets\n",
    "happy_sentences = sent_tokenize(happy_content)\n",
    "happy_tweets = pd.DataFrame(happy_sentences, columns=['text'])\n",
    "happy_tweets['sentiment'] = 1\n",
    "sad_sentences = sent_tokenize(sad_content)\n",
    "sad_tweets = pd.DataFrame(sad_sentences, columns=['text'])\n",
    "sad_tweets['sentiment'] = -1\n",
    "neutral_sentences = sent_tokenize(neutral_content)\n",
    "neutral_tweets = pd.DataFrame(neutral_sentences, columns=['text'])\n",
    "neutral_tweets['sentiment'] = 0\n",
    "\n",
    "print(happy_tweets.head())\n",
    "print(happy_tweets.shape)\n",
    "print(sad_tweets.head())\n",
    "print(sad_tweets.shape)\n",
    "print(neutral_tweets.head())\n",
    "print(neutral_tweets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  merge and clean the data\n",
    "in this step we'll clean the data by removing the duplicates and stop words, which are often meaningless words that can add noise to the dataset rather than meaningful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  sentiment\n",
      "0  An inspiration in all aspects: Fashion, fitnes...          1\n",
      "1  :)KISSES TheFashionIcon,Apka Apna Awam Ka Chan...          1\n",
      "2  Can you donate?,Omg he... kissed... him crying...          1\n",
      "3  love love happy,thanks happy,C'mon Tweeps, Joi...          1\n",
      "4                                Do spread the word.          1\n",
      "(1628, 2)\n"
     ]
    }
   ],
   "source": [
    "merged_tweets = pd.concat([happy_tweets, sad_tweets, neutral_tweets], ignore_index=True)\n",
    "merged_tweets.dropna(inplace=True)\n",
    "merged_tweets.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(merged_tweets.head())\n",
    "print(merged_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shrooma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  sentiment\n",
      "0                       inspiration aspects: fashion          1\n",
      "1                                            fitness          1\n",
      "2        beauty personality. :)kisses thefashionicon          1\n",
      "3  apka apna awam ka channel frankline tv aam adm...          1\n",
      "4  beautiful album greatest unsung guitar genius ...          1\n",
      "(3395, 2)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# transform the text to lowercase\n",
    "merged_tweets['text'] = merged_tweets['text'].str.lower()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# remove stop words\n",
    "merged_tweets['text'] = merged_tweets['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords.words('english')]))\n",
    "\n",
    "# remove duplicates\n",
    "merged_tweets.drop_duplicates(subset=['text'], inplace=True)\n",
    "merged_tweets.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(merged_tweets.head())\n",
    "print(merged_tweets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split the data with stratification\n",
    "making a split on the train and test (20%) datasets with stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1302, 2)\n",
      "(326, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_tweets, test_tweets = train_test_split(merged_tweets,\n",
    "                                             test_size=0.2,\n",
    "                                             stratify=merged_tweets['sentiment'],\n",
    "                                             random_state=42)\n",
    "\n",
    "print(train_tweets.shape)\n",
    "print(test_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset:\n",
      "sentiment\n",
      " 0    0.420147\n",
      " 1    0.360565\n",
      "-1    0.219287\n",
      "Name: proportion, dtype: float64\n",
      "Train dataset:\n",
      "sentiment\n",
      " 0    0.420123\n",
      " 1    0.360215\n",
      "-1    0.219662\n",
      "Name: proportion, dtype: float64\n",
      "Test dataset:\n",
      "sentiment\n",
      " 0    0.420245\n",
      " 1    0.361963\n",
      "-1    0.217791\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# check the distribution of the sentiment in the original, train and test datasets\n",
    "print(\"Original dataset:\")\n",
    "print(merged_tweets['sentiment'].value_counts(normalize=True))\n",
    "print(\"Train dataset:\")\n",
    "print(train_tweets['sentiment'].value_counts(normalize=True))\n",
    "print(\"Test dataset:\")\n",
    "print(test_tweets['sentiment'].value_counts(normalize=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess and vectorize the data\n",
    "\n",
    "in this step i'll prepare multiple datasets, each dataset is a combination of a preprocessing approach and a vectorization approach.\n",
    "\n",
    "-- preprocessing approaches :\n",
    "- text tokenization\n",
    "- stemming\n",
    "- lemmatization\n",
    "- stemming + mispellings correction\n",
    "- lemmatization + mispellings correction\n",
    "\n",
    "-- vectorization approaches :\n",
    "- binary vectorization\n",
    "- word counts\n",
    "- tf-idf\n",
    "- word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
